{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers faiss-gpu openai"
      ],
      "metadata": {
        "id": "ShIDJziFxOXu"
      },
      "id": "ShIDJziFxOXu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l-d6JpEDL4Ag",
      "metadata": {
        "id": "l-d6JpEDL4Ag"
      },
      "outputs": [],
      "source": [
        "# Library import\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import requests\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Numerical Result\n",
        "def extract_numerical_answer(text):\n",
        "    # Look for patterns like \"Final answer: X\" or \"The answer is X\" at the end of the text\n",
        "    match = re.search(r'(?:final answer|the answer is)[:\\s]*([+-]?\\d*\\.?\\d+)', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return float(match.group(1))\n",
        "    else:\n",
        "        # If no clear final answer, look for the last number in the text\n",
        "        numbers = re.findall(r'[+-]?\\d*\\.?\\d+', text)\n",
        "        return float(numbers[-1]) if numbers else None\n",
        "\n",
        "# LLM API call\n",
        "def llm_call(prompt):\n",
        "  client = OpenAI(api_key = openAi_key)\n",
        "  response = client.chat.completions.create(\n",
        "        model = \"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature = 0.0)\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# Load Test Dataset\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "test_df.columns = [\"problem_id\",\t\"question\"]\n",
        "\n",
        "# Load Train Dataset\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "train_df.columns = [\"problem_id\",\t\"question\",\t\"answer\"]\n",
        "\n",
        "\n",
        "# Instantiate Transformer model\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Generate embeddings for the training questions\n",
        "train_embeddings = model.encode(train_df['question'].tolist(), convert_to_tensor=True).numpy()\n",
        "\n",
        "# Dimension of embeddings\n",
        "d = train_embeddings.shape[1]\n",
        "\n",
        "# Create FAISS index\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(train_embeddings)\n",
        "\n",
        "# Generate embeddings for the test questions\n",
        "test_embeddings = model.encode(test_df['question'].tolist(), convert_to_tensor=True).numpy()\n",
        "\n",
        "# Search the index for the top k most similar questions\n",
        "k = 5\n",
        "distances, indices = index.search(test_embeddings, k)\n",
        "\n",
        "# Retrieve the top k most similar questions and their answers\n",
        "retrieved_contexts = []\n",
        "for idx_list in indices:\n",
        "    retrieved_contexts.append(train_df.iloc[idx_list])\n",
        "\n",
        "\n",
        "def openai_llm(query, contexts):\n",
        "\n",
        "    context_texts = \"\\n\".join(contexts)\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "        Role:\n",
        "        You are a Maths Professor with exceptional mathematical reasoning and problem-solving capabilities, specialised in solving tricky maths problems.\n",
        "        Your task is to accurately analyze and solve intricate mathematical QUESTION, demonstrating a deep understanding of mathematical concepts\n",
        "        and a strong ability to apply logical and analytical reasoning strategies in solving the maths QUESTION and arriving at an Answer.\n",
        "\n",
        "        Instruction:\n",
        "        1. Carefully read and comprehend the problem statement provided in the QUESTION.\n",
        "        2. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
        "        3. At the end, provide an \"Answer\" to the QUESTION, where you will state only the final numerical answer, without any additional text or narrative.\n",
        "\n",
        "        QUESTION: {question}\n",
        "\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "        \"\"\".strip()\n",
        "\n",
        "    prompt = prompt_template.format(question=query, context=context_texts).strip()\n",
        "\n",
        "    result = llm_call(prompt)\n",
        "    return result\n",
        "\n",
        "\n",
        "def openai_llm_parse(question, answer, contexts):\n",
        "\n",
        "    context_texts = \"\\n\".join(contexts)\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "        Role:\n",
        "        You are a Maths Professor with exceptional mathematical reasoning and problem-solving capabilities, specialised in solving tricky maths problems.\n",
        "        Your task is to accurately analyze an intricate mathematical QUESTION, and compare it with the ANSWER provided and fix the ANSWER if needed.\n",
        "\n",
        "        Instruction:\n",
        "        1. Carefully read and comprehend the problem statement provided in the QUESTION.\n",
        "        2. Carefully read and comprehend the solution provided by the ANSWER.\n",
        "        3. Please analyze the ANSWER for the given QUESTION\n",
        "        3. Fix the ANSWER to the QUESTION based on the CONTEXT from the FAQ database if needed.\n",
        "        4. Provide a final Answer.\n",
        "        5. Your response should end in the format: 'Hence, the final answer is [numeric string]'.\n",
        "\n",
        "\n",
        "        QUESTION: {question}\n",
        "        ANSWER: {answer}\n",
        "\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "        \"\"\".strip()\n",
        "\n",
        "    prompt = prompt_template.format(question = question, answer = answer, context=context_texts).strip()\n",
        "\n",
        "    result = llm_call(prompt)\n",
        "    return result\n",
        "\n",
        "# RAG Main body\n",
        "answers = []\n",
        "for i, test_question in enumerate(test_df['question']):\n",
        "    contexts = [f\"Q: {q}\\nA: {a}\" for q, a in zip(retrieved_contexts[i]['question'], retrieved_contexts[i]['answer'])]\n",
        "    answer = openai_llm(test_question, contexts)\n",
        "\n",
        "    #Use LLM to Verifiy the answer\n",
        "    parse_answer = openai_llm_parse(test_question, answer, contexts)\n",
        "    answers.append(parse_answer)\n",
        "\n",
        "# Add the answers to the test dataframe\n",
        "test_df['res_answer'] = answers\n",
        "\n",
        "# Extract Numerical Answer\n",
        "test_df['answer'] = test_df['res_answer'].apply(extract_numerical_answer)\n",
        "\n",
        "# Save submission file\n",
        "submission = test_df[['problem_id', 'answer']]\n",
        "submission.to_csv('notebook_submission.csv', index=False)"
      ],
      "metadata": {
        "id": "5qD1iLD9PptI"
      },
      "id": "5qD1iLD9PptI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}